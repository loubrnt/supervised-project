## üìä 1. Benchmark Global et M√©thodologie

### Protocole d'Entra√Ænement
Tous les mod√®les ont suivi le m√™me pipeline rigoureux pour assurer une comparaison √©quitable :
* **Partitionnement** : 80% Entra√Ænement / 20% Test.
* **Validation Crois√©e** : Stratified K-Fold (5 splits) pour valider la robustesse.
* **Gestion du D√©s√©quilibre** : Application d'un poids de classe positif calcul√© dynamiquement (`scale_pos_weight` ‚âà 6.04). Cela force le mod√®le √† accorder 6 fois plus d'importance aux accidents graves.
* **Optimisation** : Recherche d'hyperparam√®tres via `GridSearchCV`.

### Tableau Comparatif des Performances

| Mod√®le | Score ROC AUC (CV) | Temps d'Entra√Ænement | Vrais Positifs (D√©tect√©s) | Verdict |
| :--- | :---: | :---: | :---: | :--- |
| **XGBoost** | **0.6264** | **~9 min** (532s) | 73,552 | üèÜ **Meilleur Mod√®le** |
| **CatBoost** | 0.6261 | ~27 min (1643s) | **74,168** | ü•à Excellent rappel, mais lent |
| **Decision Tree** | 0.6193 | ~18 min (1063s) | 70,267 | ü•â Baseline de r√©f√©rence |

> **Analyse** : **XGBoost** remporte la comp√©tition. Il offre le meilleur score AUC tout en √©tant 3 fois plus rapide que CatBoost. Bien que CatBoost d√©tecte l√©g√®rement plus d'accidents graves (+600 cas), son co√ªt computationnel est prohibitif pour un gain marginal.

---

## üöÄ 2. Analyse Approfondie : XGBoost (Champion)

Le mod√®le XGBoost (eXtreme Gradient Boosting) a d√©montr√© la meilleure capacit√© de g√©n√©ralisation.

**Meilleurs Hyperparam√®tres trouv√©s :**
* `learning_rate`: 0.1
* `n_estimators`: 200
* `max_depth`: None (G√©r√© par l'algorithme)

### A. Matrice de Confusion
La matrice de confusion nous permet de visualiser les erreurs du mod√®le sur le jeu de test.

![Matrice Confusion XGBoost](../pictures/xgboost_confusion_matrix.png)

**Interpr√©tation des chiffres :**
* **Vrais Positifs (73 552)** : Le mod√®le a correctement identifi√© plus de 73 000 accidents graves. C'est le chiffre critique pour sauver des vies.
* **Faux Positifs (293 131)** : Le mod√®le a class√© "Graves" des accidents qui √©taient en r√©alit√© "L√©gers".
    * *Pourquoi est-ce √©lev√© ?* C'est une cons√©quence volontaire du poids de classe (6.04). Dans un contexte de secours d'urgence, **il vaut mieux envoyer une ambulance pour rien (Faux Positif) que de ne pas l'envoyer sur un accident mortel (Faux N√©gatif)**. Le mod√®le est "prudemment pessimiste".
* **Faux N√©gatifs (63 786)** : Accidents graves manqu√©s. Ce chiffre est minimis√© autant que possible par l'optimisation.

### B. Courbe ROC
La courbe ROC illustre la performance du classifieur √† diff√©rents seuils de discrimination.

![Courbe ROC XGBoost](../pictures/xgboost_roc_curve.png)

**Analyse :**
L'aire sous la courbe (AUC) de **0.6264** indique que le mod√®le a une capacit√© discriminante sup√©rieure √† l'al√©atoire (0.5). La courbe monte rapidement au d√©but, ce qui signifie que le mod√®le est efficace pour identifier les cas les plus √©vidents de gravit√© avec peu de faux positifs initiaux.

### C. Importance des Variables (Feature Importance)
Quels facteurs influencent le plus la d√©cision de XGBoost ?

![Feature Importance XGBoost](../pictures/xgboost_features_importance.png)

**Analyse M√©tier :**
1.  **`road_type_6` (Route √† chauss√©e unique)** : C'est de loin le facteur n¬∞1 (~23%). Les routes de campagne bidirectionnelles sont statistiquement les plus meurtri√®res (chocs frontaux).
2.  **`urban_or_rural_area_1` (Zone Urbaine)** : Facteur protecteur majeur (~23%). En ville, la vitesse r√©duite diminue drastiquement la gravit√©.
3.  **`light_conditions`** : La luminosit√© (jour/nuit) arrive en 3√®me position (~8%), confirmant que la visibilit√© est cruciale.

---

## üê± 3. Analyse Approfondie : CatBoost

CatBoost (Categorical Boosting) est r√©put√© pour sa gestion native des cat√©gories, mais s'est av√©r√© lourd √† entra√Æner sur ce dataset volumineux.

**Meilleurs Hyperparam√®tres trouv√©s :**
* `learning_rate`: 0.1
* `n_estimators`: 200

### A. Matrice de Confusion

![Matrice Confusion CatBoost](../pictures/catboost_confusion_matrix.png)

**Comparaison avec XGBoost :**
* **Vrais Positifs (74 168)** : CatBoost est l√©g√®rement plus "sensible". Il a d√©tect√© **616 accidents graves de plus** que XGBoost.
* **Faux Positifs (297 378)** : En contrepartie, il a g√©n√©r√© environ 4 000 fausses alertes suppl√©mentaires.
* L'√©quilibre est tr√®s similaire, mais CatBoost favorise l√©g√®rement plus le Rappel (Recall) au d√©triment de la Pr√©cision.

### B. Courbe ROC

![Courbe ROC CatBoost](../pictures/catboost_roc_curve.png)

**Analyse :**
Avec une AUC de **0.6261**, la courbe est quasi superposable √† celle de XGBoost. La performance pr√©dictive pure est √©quivalente. La diff√©rence principale r√©side donc dans le temps de calcul (27 min vs 9 min).

### C. Importance des Variables

![Feature Importance CatBoost](../pictures/catboost_features_importance.png)

**Divergence Notable :**
L'analyse des features de CatBoost est tr√®s diff√©rente de celle de XGBoost :
* **`road_type`** reste premier (~25%), confirmant la robustesse de ce facteur.
* **`speed_limit` (Limite de vitesse)** appara√Æt en 2√®me position avec une importance massive de **17%** (contre seulement 5% pour XGBoost).
* Cela sugg√®re que CatBoost arrive mieux √† capturer la relation lin√©aire entre la limite de vitesse autoris√©e et la gravit√© du choc cin√©tique.

---

## üå≥ 4. Analyse Approfondie : Decision Tree

L'arbre de d√©cision sert de "Baseline". Il est plus simple et donc plus interpr√©table, mais capture moins de nuances complexes que les m√©thodes de Boosting.

**Meilleurs Hyperparam√®tres trouv√©s :**
* `criterion`: 'entropy' (Gain d'information)
* `max_depth`: 10 (Profondeur contrainte pour √©viter le surapprentissage)

### A. Matrice de Confusion

![Matrice Confusion Decision Tree](../pictures/decision_tree_confusion_matrix.png)

**Analyse :**
* **Vrais Positifs (70 267)** : Le score le plus bas des trois mod√®les. Il "rate" environ 3 300 accidents graves compar√© √† XGBoost.
* **Faux Positifs (279 151)** : Il g√©n√®re moins de fausses alertes.
* Ce mod√®le est plus conservateur. Dans notre contexte o√π la non-d√©tection est grave, ce comportement est moins souhaitable.

### B. Courbe ROC

![Courbe ROC Decision Tree](../pictures/decision_tree_roc_curve.png)

**Analyse :**
L'AUC de **0.6193** est inf√©rieure aux m√©thodes de boosting. La forme de la courbe est moins "bomb√©e", ce qui traduit une moins bonne s√©paration entre les classes graves et l√©g√®res.

### C. Importance des Variables

![Feature Importance Decision Tree](../pictures/decision_tree_features_importance.png)

**Analyse :**
L'arbre de d√©cision a une structure hi√©rarchique qui privil√©gie des variables diff√©rentes :
1.  **`urban_or_rural_area_1`** : Premier crit√®re de split (~24%).
2.  **`light_conditions`** : Deuxi√®me crit√®re (~13%).
3.  **`road_type_6`** : Troisi√®me crit√®re (~11%).

---

## ‚úÖ Conclusion et Recommandations

Au terme de cette phase d'entra√Ænement, nous pouvons tirer les conclusions suivantes :

1.  **Choix du Mod√®le** : Nous retenons **XGBoost** pour la mise en production. Il offre le meilleur √©quilibre performance/rapidit√©.
2.  **Facteurs de Risque** : Quel que soit le mod√®le, trois variables reviennent syst√©matiquement comme pr√©dicteurs cl√©s : **le type de route (campagne), la zone (urbaine/rurale) et la luminosit√©**.
3.  **Strat√©gie de Seuil** : Les matrices de confusion montrent un grand nombre de Faux Positifs. Pour une application r√©elle, il serait pertinent d'ajouter une √©tape de calibration de probabilit√© ou de permettre √† l'op√©rateur de r√©gler la sensibilit√© du mod√®le (ajustement du seuil, actuellement √† 0.5).